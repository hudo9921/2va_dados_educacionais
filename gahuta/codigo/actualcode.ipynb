{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39a34d92",
   "metadata": {},
   "source": [
    "## Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f126e3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hudo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from aibox.nlp.factory import get_extractor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0cbaf6",
   "metadata": {},
   "source": [
    "## Importação dos dados e conversao de Json para dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a8d364",
   "metadata": {},
   "source": [
    "Prova 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8d559aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dados/provas_json/provas_concat/2019_d1.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2019_d1 = pd.DataFrame(data)\n",
    "with open(\"../dados/provas_json/provas_concat/2019_d2.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2019_d2 = pd.DataFrame(data)\n",
    "\n",
    "prova2019 = pd.concat([prova2019_d1,prova2019_d2],ignore_index=True)\n",
    "\n",
    "itens2019 = pd.read_csv(\"../dados/processados/ITENS_PROVA_2019_AZUL.csv\")\n",
    "itens2019.head()\n",
    "\n",
    "full2019 = pd.merge(itens2019,prova2019,\n",
    "                    left_on='CO_POSICAO',\n",
    "                    right_on='number',\n",
    "                    how='left')\n",
    "\n",
    "df2019 = full2019[['text','NU_PARAM_B','has_img']]\n",
    "\n",
    "df2019 = df2019.dropna(subset=[\"NU_PARAM_B\", \"text\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ec78b",
   "metadata": {},
   "source": [
    "Prova 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94f90bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dados/provas_json/provas_concat/2020_d1.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2020_d1 = pd.DataFrame(data)\n",
    "with open(\"../dados/provas_json/provas_concat/2020_d2.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2020_d2 = pd.DataFrame(data)\n",
    "\n",
    "prova2020 = pd.concat([prova2020_d1,prova2020_d2],ignore_index=True)\n",
    "\n",
    "itens2020 = pd.read_csv(\"../dados/processados/ITENS_PROVA_2020_AZUL.csv\")\n",
    "itens2020.head()\n",
    "\n",
    "full2020 = pd.merge(itens2020,prova2020,\n",
    "                    left_on='CO_POSICAO',\n",
    "                    right_on='number',\n",
    "                    how='left')\n",
    "\n",
    "df2020 = full2020[['text','NU_PARAM_B','has_img']]\n",
    "\n",
    "df2020 = df2020.dropna(subset=[\"NU_PARAM_B\", \"text\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e2a75a",
   "metadata": {},
   "source": [
    "Prova 2021 (ta dando bo na importacao n sei pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab9529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>NU_PARAM_B</th>\n",
       "      <th>has_img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>]\u0011\u0005 \b\f\u0011\u0005 \u000f\u0003^\u0005 \b\u0005 \u000f\b\u0012\\n\u0003\u000f\u0005 \u0003&amp;\u0006\u0011\u0006\u0006\u0011\u0010\u0005 ^\\n\u00128\u0005 \u00128\u0011...</td>\n",
       "      <td>0.61130</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The British (serves 60 million)$\bf\u0011\u0005\u0006\u0003\u0004\u0011\u0005'\\n\u0002\u0012...</td>\n",
       "      <td>0.63692</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Becoming`\b\u0002f\u0005 \\n\u000f\u0005 \u00128\u0011\u0005 \b\u000f\u0002\u0011\u0006\u0012\f\b\\t\u0005 8\u0003\u0004\u0011\\t\b\u000f\u0010\u0005...</td>\n",
       "      <td>0.41261</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fb')=FF4\u0005&lt;\\n\u0006(\u0003\u000f5\u001d\u0011\\t\u0005\u0011\u0004\u0015\u0005^^^4\u000f\u0011^_\u0003\ff\u0011\f4\u0002\u0003\u00044\u0005;...</td>\n",
       "      <td>0.34872</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Exterior: Between The Museums — Day\"=!b&gt;=;\u0004\u0011\f\\...</td>\n",
       "      <td>0.49582</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  NU_PARAM_B  has_img\n",
       "0  ]\u0011\u0005 \b\n",
       "\u0011\u0005 \u000f\u0003^\u0005 \b\u0005 \u000f\b\u0012\\n\u0003\u000f\u0005 \u0003&\u0006\u0011\u0006\u0006\u0011\u0010\u0005 ^\\n\u00128\u0005 \u00128\u0011...     0.61130      0.0\n",
       "1  The British (serves 60 million)$\bf\u0011\u0005\u0006\u0003\u0004\u0011\u0005'\\n\u0002\u0012...     0.63692      0.0\n",
       "2  Becoming`\b\u0002f\u0005 \\n\u000f\u0005 \u00128\u0011\u0005 \b\u000f\u0002\u0011\u0006\u0012\n",
       "\b\\t\u0005 8\u0003\u0004\u0011\\t\b\u000f\u0010\u0005...     0.41261      0.0\n",
       "3  Fb')=FF4\u0005<\\n\u0006(\u0003\u000f5\n",
       "\u0011\\t\u0005\u0011\u0004\u0015\u0005^^^4\u000f\u0011^_\u0003\n",
       "f\u0011\n",
       "4\u0002\u0003\u00044\u0005;...     0.34872      1.0\n",
       "4  Exterior: Between The Museums — Day\"=!b>=;\u0004\u0011\n",
       "\\...     0.49582      0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../dados/provas_json/provas_concat/2021_d1.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2019_d1 = pd.DataFrame(data)\n",
    "with open(\"../dados/provas_json/provas_concat/2021_d2.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2019_d2 = pd.DataFrame(data)\n",
    "\n",
    "prova2023 = pd.concat([prova2019_d1,prova2019_d2],ignore_index=True)\n",
    "\n",
    "itens2023 = pd.read_csv(\"../dados/processados/ITENS_PROVA_2019_AZUL.csv\")\n",
    "itens2023.head()\n",
    "\n",
    "full2023 = pd.merge(itens2023,prova2023,\n",
    "                    left_on='CO_POSICAO',\n",
    "                    right_on='number',\n",
    "                    how='left')\n",
    "\n",
    "df2023 = full2023[['text','NU_PARAM_B','has_img']]\n",
    "\n",
    "df2023.head()\n",
    "\n",
    "##TA EXTRAINDO MAL N SEI PQ DEIXA 2021 PRA LA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac09da3",
   "metadata": {},
   "source": [
    "Prova 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7302cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dados/provas_json/provas_concat/2022_d1.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2022_d1 = pd.DataFrame(data)\n",
    "with open(\"../dados/provas_json/provas_concat/2022_d2.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2022_d2 = pd.DataFrame(data)\n",
    "\n",
    "prova2022 = pd.concat([prova2022_d1,prova2022_d2],ignore_index=True)\n",
    "\n",
    "itens2022 = pd.read_csv(\"../dados/processados/ITENS_PROVA_2022_AZUL.csv\")\n",
    "itens2022.head()\n",
    "\n",
    "full2022 = pd.merge(itens2022,prova2022,\n",
    "                    left_on='CO_POSICAO',\n",
    "                    right_on='number',\n",
    "                    how='left')\n",
    "\n",
    "df2022 = full2022[['text','NU_PARAM_B','has_img']]\n",
    "\n",
    "df2022 = df2022.dropna(subset=[\"NU_PARAM_B\", \"text\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85869fc",
   "metadata": {},
   "source": [
    "Prova 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9049e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dados/provas_json/provas_concat/2023_d1.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2023_d1 = pd.DataFrame(data)\n",
    "with open(\"../dados/provas_json/provas_concat/2023_d2.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2023_d2 = pd.DataFrame(data)\n",
    "\n",
    "prova2023 = pd.concat([prova2023_d1,prova2023_d2],ignore_index=True)\n",
    "\n",
    "itens2023 = pd.read_csv(\"../dados/processados/ITENS_PROVA_2023_AZUL.csv\")\n",
    "itens2023.head()\n",
    "\n",
    "full2023 = pd.merge(itens2023,prova2023,\n",
    "                    left_on='CO_POSICAO',\n",
    "                    right_on='number',\n",
    "                    how='left')\n",
    "\n",
    "df2023 = full2023[['text','NU_PARAM_B','has_img']]\n",
    "\n",
    "df2023 = df2023.dropna(subset=[\"NU_PARAM_B\", \"text\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6252d9",
   "metadata": {},
   "source": [
    "## Extraindo Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "566112de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aibox.nlp.resources]: download dictionary/morph-checker.v1: 100%|██████████| 1.32k/1.32k [00:00<00:00, 675kB/s]\n",
      "[aibox.nlp.resources]: download dictionary/nominal-regency.v1: 100%|██████████| 1.46k/1.46k [00:00<00:00, 500kB/s]\n",
      "[aibox.nlp.resources]: download dictionary/verb-conjugation.v1: 100%|██████████| 52.6k/52.6k [00:00<00:00, 342kB/s]\n",
      "[aibox.nlp.resources]: download dictionary/liwc-dictionary.v1: 100%|██████████| 390k/390k [00:00<00:00, 666kB/s]\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    \"readabilityBR\",\n",
    "    \"textualSimplicityBR\",\n",
    "    \"syntacticComplexityBR\",\n",
    "    \"lexicalDiversityBR\",\n",
    "    \"connectivesV2BR\",\n",
    "    \"referentialCohesionBR\",\n",
    "    \"sequentialCohesionBR\",\n",
    "    \"regencyBR\",\n",
    "    \"conjugationBR\",\n",
    "    \"liwcBR\"\n",
    "]\n",
    "extractorRead = get_extractor([\"readabilityBR\"])\n",
    "extractorSimp = get_extractor([\"textualSimplicityBR\"])\n",
    "extractorComp = get_extractor([\"syntacticComplexityBR\"])\n",
    "extractorDiver = get_extractor([\"lexicalDiversityBR\"])\n",
    "extractorCon = get_extractor([\"connectivesV2BR\"])\n",
    "extractorReCoh = get_extractor([\"referentialCohesionBR\"])\n",
    "extractorSeCoh = get_extractor([\"sequentialCohesionBR\"])\n",
    "extractorReg = get_extractor([\"regencyBR\"])\n",
    "extractorConju = get_extractor([\"conjugationBR\"])\n",
    "extractorliwc = get_extractor([\"liwcBR\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4681e5",
   "metadata": {},
   "source": [
    "extraindo 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51470855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 164/172 [22:17<04:10, 31.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction error: LexicalDiversityFeatures.__init__() got an unexpected keyword argument 'verb'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 172/172 [23:28<00:00,  8.19s/it]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "def extract_all(text):\n",
    "    try:\n",
    "        result = {}\n",
    "        result.update(extractorRead.extract(text).as_dict())\n",
    "        result.update(extractorSimp.extract(text).as_dict())\n",
    "        result.update(extractorComp.extract(text).as_dict())\n",
    "        result.update(extractorDiver.extract(text).as_dict())\n",
    "        result.update(extractorCon.extract(text).as_dict())\n",
    "        result.update(extractorReCoh.extract(text).as_dict())\n",
    "        result.update(extractorSeCoh.extract(text).as_dict())\n",
    "        result.update(extractorReg.extract(text).as_dict())\n",
    "        result.update(extractorConju.extract(text).as_dict())\n",
    "        result.update(extractorliwc.extract(text).as_dict())\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction error: {e}\")\n",
    "        return {}\n",
    "\n",
    "# 4. Apply only to the 'text' column\n",
    "df2019[\"feat_dict\"] = df2019[\"text\"].progress_apply(extract_all)\n",
    "\n",
    "# 5. Flatten the dictionaries to columns\n",
    "df_feats = pd.json_normalize(df2019[\"feat_dict\"])\n",
    "\n",
    "# 6. Merge with the original columns you want to keep\n",
    "df2019_final = pd.concat(\n",
    "    [df2019.drop(columns=\"feat_dict\"), df_feats], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050de28b",
   "metadata": {},
   "source": [
    "extraindo 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2101a356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 172/175 [22:47<00:30, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction error: LexicalDiversityFeatures.__init__() got an unexpected keyword argument 'verb'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [23:13<00:00,  7.96s/it]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['feat_dict'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     24\u001b[39m df_feats = pd.json_normalize(df2020[\u001b[33m\"\u001b[39m\u001b[33mfeat_dict\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# 6. Merge with the original columns you want to keep\u001b[39;00m\n\u001b[32m     27\u001b[39m df2020_final = pd.concat(\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     [\u001b[43mdf2022\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeat_dict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, df_feats], axis=\u001b[32m1\u001b[39m\n\u001b[32m     29\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hudo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5434\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5435\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5442\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5443\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5444\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5445\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5446\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5579\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5580\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5581\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5583\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5587\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5588\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5589\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hudo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4786\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4788\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4790\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4791\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hudo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4828\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4829\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4830\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4831\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4833\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4834\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hudo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7068\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7069\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7070\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7071\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: \"['feat_dict'] not found in axis\""
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "def extract_all(text):\n",
    "    try:\n",
    "        result = {}\n",
    "        result.update(extractorRead.extract(text).as_dict())\n",
    "        result.update(extractorSimp.extract(text).as_dict())\n",
    "        result.update(extractorComp.extract(text).as_dict())\n",
    "        result.update(extractorDiver.extract(text).as_dict())\n",
    "        result.update(extractorCon.extract(text).as_dict())\n",
    "        result.update(extractorReCoh.extract(text).as_dict())\n",
    "        result.update(extractorSeCoh.extract(text).as_dict())\n",
    "        result.update(extractorReg.extract(text).as_dict())\n",
    "        result.update(extractorConju.extract(text).as_dict())\n",
    "        result.update(extractorliwc.extract(text).as_dict())\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction error: {e}\")\n",
    "        return {}\n",
    "\n",
    "# 4. Apply only to the 'text' column\n",
    "df2020[\"feat_dict\"] = df2020[\"text\"].progress_apply(extract_all)\n",
    "\n",
    "# 5. Flatten the dictionaries to columns\n",
    "df_feats = pd.json_normalize(df2020[\"feat_dict\"])\n",
    "\n",
    "# 6. Merge with the original columns you want to keep\n",
    "df2020_final = pd.concat(\n",
    "    [df2020.drop(columns=\"feat_dict\"), df_feats], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1855e0",
   "metadata": {},
   "source": [
    "extraindo 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "071db3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 137/171 [20:22<05:03,  8.92s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction error: LexicalDiversityFeatures.__init__() got an unexpected keyword argument 'verb'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 152/171 [25:31<07:06, 22.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction error: LexicalDiversityFeatures.__init__() got an unexpected keyword argument 'verb'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [29:21<00:00, 10.30s/it]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "def extract_all(text):\n",
    "    try:\n",
    "        result = {}\n",
    "        result.update(extractorRead.extract(text).as_dict())\n",
    "        result.update(extractorSimp.extract(text).as_dict())\n",
    "        result.update(extractorComp.extract(text).as_dict())\n",
    "        result.update(extractorDiver.extract(text).as_dict())\n",
    "        result.update(extractorCon.extract(text).as_dict())\n",
    "        result.update(extractorReCoh.extract(text).as_dict())\n",
    "        result.update(extractorSeCoh.extract(text).as_dict())\n",
    "        result.update(extractorReg.extract(text).as_dict())\n",
    "        result.update(extractorConju.extract(text).as_dict())\n",
    "        result.update(extractorliwc.extract(text).as_dict())\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction error: {e}\")\n",
    "        return {}\n",
    "\n",
    "# 4. Apply only to the 'text' column\n",
    "df2022[\"feat_dict\"] = df2022[\"text\"].progress_apply(extract_all)\n",
    "\n",
    "# 5. Flatten the dictionaries to columns\n",
    "df_feats = pd.json_normalize(df2022[\"feat_dict\"])\n",
    "\n",
    "# 6. Merge with the original columns you want to keep\n",
    "df2022_final = pd.concat(\n",
    "    [df2022.drop(columns=\"feat_dict\"), df_feats], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9713d4d",
   "metadata": {},
   "source": [
    "extraindo 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4452695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 177/177 [19:21<00:00,  6.56s/it]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "def extract_all(text):\n",
    "    try:\n",
    "        result = {}\n",
    "        result.update(extractorRead.extract(text).as_dict())\n",
    "        result.update(extractorSimp.extract(text).as_dict())\n",
    "        result.update(extractorComp.extract(text).as_dict())\n",
    "        result.update(extractorDiver.extract(text).as_dict())\n",
    "        result.update(extractorCon.extract(text).as_dict())\n",
    "        result.update(extractorReCoh.extract(text).as_dict())\n",
    "        result.update(extractorSeCoh.extract(text).as_dict())\n",
    "        result.update(extractorReg.extract(text).as_dict())\n",
    "        result.update(extractorConju.extract(text).as_dict())\n",
    "        result.update(extractorliwc.extract(text).as_dict())\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction error: {e}\")\n",
    "        return {}\n",
    "\n",
    "# 4. Apply only to the 'text' column\n",
    "df2023[\"feat_dict\"] = df2023[\"text\"].progress_apply(extract_all)\n",
    "\n",
    "# 5. Flatten the dictionaries to columns\n",
    "df_feats = pd.json_normalize(df2023[\"feat_dict\"])\n",
    "\n",
    "# 6. Merge with the original columns you want to keep\n",
    "df2023_final = pd.concat(\n",
    "    [df2023.drop(columns=\"feat_dict\"), df_feats], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9facacf2",
   "metadata": {},
   "source": [
    "## Treinando"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b06e7",
   "metadata": {},
   "source": [
    "Instanciando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "817a7961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models: 100%|██████████| 6/6 [00:02<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  R2 Score\n",
      "LinearRegression -0.735081\n",
      "           Ridge -0.479189\n",
      "           Lasso  0.054075\n",
      "    RandomForest -0.118427\n",
      "GradientBoosting -0.128528\n",
      "             SVR  0.056603\n",
      "Best model: SVR with R2 Score: 0.0566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare train/test split (e.g., 80/20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train = pd.concat([df2019_final, df2020_final, df2022_final], ignore_index=True)\n",
    "df_train.dropna(inplace=True)\n",
    "X = df_train.drop(columns=[\"NU_PARAM_B\", \"text\"])\n",
    "y = df_train[\"NU_PARAM_B\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def train_and_select_model(X_train, y_train, X_test, y_test):\n",
    "    models = {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"Ridge\": Ridge(),\n",
    "        \"Lasso\": Lasso(),\n",
    "        \"RandomForest\": RandomForestRegressor(),\n",
    "        \"GradientBoosting\": GradientBoostingRegressor(),\n",
    "        \"SVR\": SVR(),\n",
    "    }\n",
    "    results = {}\n",
    "    for name, model in tqdm(models.items(), desc=\"Training models\"):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        score = r2_score(y_test, y_pred)\n",
    "        results[name] = score\n",
    "    # Output results as a table\n",
    "    results_df = pd.DataFrame(list(results.items()), columns=[\"Model\", \"R2 Score\"])\n",
    "    print(results_df.to_string(index=False))\n",
    "    best_model_name = max(results, key=results.get)\n",
    "    best_model = models[best_model_name]\n",
    "    print(f\"Best model: {best_model_name} with R2 Score: {results[best_model_name]:.4f}\")\n",
    "    return best_model, results\n",
    "\n",
    "best_model, results = train_and_select_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b81714b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on 2023 data: 0.0787\n"
     ]
    }
   ],
   "source": [
    "# Predict on 2023\n",
    "X_2023 = df2023_final.drop(columns=[\"NU_PARAM_B\", \"text\"]).dropna()\n",
    "y_2023_true = df2023_final.loc[X_2023.index, \"NU_PARAM_B\"]\n",
    "y_2023_pred = best_model.predict(X_2023)\n",
    "\n",
    "# Avaliação do modelo no conjunto de 2023\n",
    "score_2023 = r2_score(y_2023_true, y_2023_pred)\n",
    "print(f\"R2 score on 2023 data: {score_2023:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "758e92af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj_stem_ovl              0.327921\n",
      "adj_cw_ovl                0.317152\n",
      "stem_ovl                  0.303348\n",
      "adj_arg_ovl               0.299689\n",
      "local_coh_pw_dist         0.292564\n",
      "                            ...   \n",
      "token_var_idx            -0.194358\n",
      "honore_statistics        -0.228937\n",
      "lexical_diversity_mtld   -0.238043\n",
      "lexical_diversity        -0.250531\n",
      "guiraud_index            -0.256376\n",
      "Length: 145, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hudo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\hudo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "exclude_cols = [\"NU_PARAM_B\", \"text\"]\n",
    "feature_cols = [col for col in df_train.columns if col not in exclude_cols]\n",
    "corrs = df_train[feature_cols].corrwith(df_train[\"NU_PARAM_B\"]).dropna().sort_values(ascending=False)\n",
    "print(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a5448884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models: 100%|██████████| 6/6 [00:00<00:00, 14.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  R2 Score\n",
      "LinearRegression -0.028864\n",
      "           Ridge -0.020737\n",
      "           Lasso  0.025402\n",
      "    RandomForest -0.431638\n",
      "GradientBoosting -0.696736\n",
      "             SVR  0.004405\n",
      "Best model: Lasso with R2 Score: 0.0254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top_features = corrs.head(10).index.tolist()\n",
    "X_top = df_train[top_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=0.2, random_state=42)\n",
    "\n",
    "best_model, results = train_and_select_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "05efcc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models: 100%|██████████| 6/6 [00:00<00:00, 14.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  R2 Score\n",
      "LinearRegression -0.028864\n",
      "           Ridge -0.020737\n",
      "           Lasso  0.025402\n",
      "    RandomForest -0.379281\n",
      "GradientBoosting -0.674213\n",
      "             SVR  0.004405\n",
      "Best model: Lasso with R2 Score: 0.0254\n",
      "R2 score on 2023 data with top features: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Seleciona as top features e treina\n",
    "top_features = corrs.head(10).index.tolist()\n",
    "X_top = df_train[top_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=0.2, random_state=42)\n",
    "best_model, results = train_and_select_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Faz previsão no conjunto de 2023 usando só as top features\n",
    "X_2023_top = df2023_final[top_features].dropna()\n",
    "y_2023_true = df2023_final.loc[X_2023_top.index, \"NU_PARAM_B\"]\n",
    "y_2023_pred = best_model.predict(X_2023_top)\n",
    "\n",
    "score_2023 = r2_score(y_2023_true, y_2023_pred)\n",
    "print(f\"R2 score on 2023 data with top features: {score_2023:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
