{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39a34d92",
   "metadata": {},
   "source": [
    "## Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f126e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from aibox.nlp.factory import get_extractor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0cbaf6",
   "metadata": {},
   "source": [
    "## Importação dos dados e conversao de Json para dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a8d364",
   "metadata": {},
   "source": [
    "Prova 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8d559aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dados/provas_json/provas_concat/2019_d1.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2019_d1 = pd.DataFrame(data)\n",
    "with open(\"../dados/provas_json/provas_concat/2019_d2.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2019_d2 = pd.DataFrame(data)\n",
    "\n",
    "prova2019 = pd.concat([prova2019_d1,prova2019_d2],ignore_index=True)\n",
    "\n",
    "itens2019 = pd.read_csv(\"../dados/processados/ITENS_PROVA_2019_AZUL.csv\")\n",
    "itens2019.head()\n",
    "\n",
    "full2019 = pd.merge(itens2019,prova2019,\n",
    "                    left_on='CO_POSICAO',\n",
    "                    right_on='number',\n",
    "                    how='left')\n",
    "\n",
    "df2019 = full2019[['text','NU_PARAM_B','has_img']]\n",
    "\n",
    "df2019 = df2019.dropna(subset=[\"NU_PARAM_B\", \"text\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ec78b",
   "metadata": {},
   "source": [
    "Prova 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "94f90bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dados/provas_json/provas_concat/2020_d1.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2020_d1 = pd.DataFrame(data)\n",
    "with open(\"../dados/provas_json/provas_concat/2020_d2.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2020_d2 = pd.DataFrame(data)\n",
    "\n",
    "prova2020 = pd.concat([prova2020_d1,prova2020_d2],ignore_index=True)\n",
    "\n",
    "itens2020 = pd.read_csv(\"../dados/processados/ITENS_PROVA_2019_AZUL.csv\")\n",
    "itens2020.head()\n",
    "\n",
    "full2020 = pd.merge(itens2020,prova2020,\n",
    "                    left_on='CO_POSICAO',\n",
    "                    right_on='number',\n",
    "                    how='left')\n",
    "\n",
    "df2020 = full2020[['text','NU_PARAM_B','has_img']]\n",
    "\n",
    "df2020 = df2020.dropna(subset=[\"NU_PARAM_B\", \"text\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e2a75a",
   "metadata": {},
   "source": [
    "Prova 2021 (ta dando bo na importacao n sei pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab9529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>NU_PARAM_B</th>\n",
       "      <th>has_img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>]\u0011\u0005 \b\f\u0011\u0005 \u000f\u0003^\u0005 \b\u0005 \u000f\b\u0012\\n\u0003\u000f\u0005 \u0003&amp;\u0006\u0011\u0006\u0006\u0011\u0010\u0005 ^\\n\u00128\u0005 \u00128\u0011...</td>\n",
       "      <td>0.61130</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The British (serves 60 million)$\bf\u0011\u0005\u0006\u0003\u0004\u0011\u0005'\\n\u0002\u0012...</td>\n",
       "      <td>0.63692</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Becoming`\b\u0002f\u0005 \\n\u000f\u0005 \u00128\u0011\u0005 \b\u000f\u0002\u0011\u0006\u0012\f\b\\t\u0005 8\u0003\u0004\u0011\\t\b\u000f\u0010\u0005...</td>\n",
       "      <td>0.41261</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fb')=FF4\u0005&lt;\\n\u0006(\u0003\u000f5\u001d\u0011\\t\u0005\u0011\u0004\u0015\u0005^^^4\u000f\u0011^_\u0003\ff\u0011\f4\u0002\u0003\u00044\u0005;...</td>\n",
       "      <td>0.34872</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Exterior: Between The Museums — Day\"=!b&gt;=;\u0004\u0011\f\\...</td>\n",
       "      <td>0.49582</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  NU_PARAM_B  has_img\n",
       "0  ]\u0011\u0005 \b\n",
       "\u0011\u0005 \u000f\u0003^\u0005 \b\u0005 \u000f\b\u0012\\n\u0003\u000f\u0005 \u0003&\u0006\u0011\u0006\u0006\u0011\u0010\u0005 ^\\n\u00128\u0005 \u00128\u0011...     0.61130      0.0\n",
       "1  The British (serves 60 million)$\bf\u0011\u0005\u0006\u0003\u0004\u0011\u0005'\\n\u0002\u0012...     0.63692      0.0\n",
       "2  Becoming`\b\u0002f\u0005 \\n\u000f\u0005 \u00128\u0011\u0005 \b\u000f\u0002\u0011\u0006\u0012\n",
       "\b\\t\u0005 8\u0003\u0004\u0011\\t\b\u000f\u0010\u0005...     0.41261      0.0\n",
       "3  Fb')=FF4\u0005<\\n\u0006(\u0003\u000f5\n",
       "\u0011\\t\u0005\u0011\u0004\u0015\u0005^^^4\u000f\u0011^_\u0003\n",
       "f\u0011\n",
       "4\u0002\u0003\u00044\u0005;...     0.34872      1.0\n",
       "4  Exterior: Between The Museums — Day\"=!b>=;\u0004\u0011\n",
       "\\...     0.49582      0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../dados/provas_json/provas_concat/2021_d1.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2019_d1 = pd.DataFrame(data)\n",
    "with open(\"../dados/provas_json/provas_concat/2021_d2.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2019_d2 = pd.DataFrame(data)\n",
    "\n",
    "prova2023 = pd.concat([prova2019_d1,prova2019_d2],ignore_index=True)\n",
    "\n",
    "itens2023 = pd.read_csv(\"../dados/processados/ITENS_PROVA_2019_AZUL.csv\")\n",
    "itens2023.head()\n",
    "\n",
    "full2023 = pd.merge(itens2023,prova2023,\n",
    "                    left_on='CO_POSICAO',\n",
    "                    right_on='number',\n",
    "                    how='left')\n",
    "\n",
    "df2023 = full2023[['text','NU_PARAM_B','has_img']]\n",
    "\n",
    "df2023.head()\n",
    "\n",
    "##TA EXTRAINDO MAL N SEI PQ DEIXA 2021 PRA LA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac09da3",
   "metadata": {},
   "source": [
    "Prova 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7302cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dados/provas_json/provas_concat/2022_d1.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2022_d1 = pd.DataFrame(data)\n",
    "with open(\"../dados/provas_json/provas_concat/2022_d2.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2022_d2 = pd.DataFrame(data)\n",
    "\n",
    "prova2022 = pd.concat([prova2022_d1,prova2022_d2],ignore_index=True)\n",
    "\n",
    "itens2022 = pd.read_csv(\"../dados/processados/ITENS_PROVA_2019_AZUL.csv\")\n",
    "itens2022.head()\n",
    "\n",
    "full2022 = pd.merge(itens2022,prova2022,\n",
    "                    left_on='CO_POSICAO',\n",
    "                    right_on='number',\n",
    "                    how='left')\n",
    "\n",
    "df2022 = full2022[['text','NU_PARAM_B','has_img']]\n",
    "\n",
    "df2022 = df2022.dropna(subset=[\"NU_PARAM_B\", \"text\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85869fc",
   "metadata": {},
   "source": [
    "Prova 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9049e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dados/provas_json/provas_concat/2023_d1.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2023_d1 = pd.DataFrame(data)\n",
    "with open(\"../dados/provas_json/provas_concat/2023_d2.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame\n",
    "prova2023_d2 = pd.DataFrame(data)\n",
    "\n",
    "prova2023 = pd.concat([prova2023_d1,prova2023_d2],ignore_index=True)\n",
    "\n",
    "itens2023 = pd.read_csv(\"../dados/processados/ITENS_PROVA_2019_AZUL.csv\")\n",
    "itens2023.head()\n",
    "\n",
    "full2023 = pd.merge(itens2023,prova2023,\n",
    "                    left_on='CO_POSICAO',\n",
    "                    right_on='number',\n",
    "                    how='left')\n",
    "\n",
    "df2023 = full2023[['text','NU_PARAM_B','has_img']]\n",
    "\n",
    "df2023 = df2023.dropna(subset=[\"NU_PARAM_B\", \"text\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6252d9",
   "metadata": {},
   "source": [
    "## Extraindo Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "566112de",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"readabilityBR\",\n",
    "    \"textualSimplicityBR\",\n",
    "    \"syntacticComplexityBR\",\n",
    "    \"lexicalDiversityBR\",\n",
    "    \"connectivesV2BR\",\n",
    "    \"referentialCohesionBR\",\n",
    "    \"sequentialCohesionBR\",\n",
    "    \"regencyBR\",\n",
    "    \"conjugationBR\",\n",
    "    \"liwcBR\"\n",
    "]\n",
    "\n",
    "extractor = get_extractor(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "51470855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [01:16<00:00,  2.28it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "def extract_all(text):\n",
    "    try:\n",
    "        return extractor.extract(text).as_dict()\n",
    "    except Exception as e:\n",
    "        # If something goes wrong (e.g., non‑Portuguese text) return empty dict\n",
    "        print(f\"Extraction error: {e}\")\n",
    "        return {}\n",
    "\n",
    "# 4. Apply only to the 'text' column\n",
    "df2023[\"feat_dict\"] = df2023[\"text\"].progress_apply(extract_all)\n",
    "\n",
    "# 5. Flatten the dictionaries to columns\n",
    "df_feats = pd.json_normalize(df2023[\"feat_dict\"])\n",
    "\n",
    "# 6. Merge with the original columns you want to keep\n",
    "df2023_final = pd.concat(\n",
    "    [df2023.drop(columns=\"feat_dict\"), df_feats], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fe1a8f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>NU_PARAM_B</th>\n",
       "      <th>has_img</th>\n",
       "      <th>adapted_dalechall</th>\n",
       "      <th>brunet_indice</th>\n",
       "      <th>flesch_indice</th>\n",
       "      <th>gunning_fox_indice</th>\n",
       "      <th>honore_statistics</th>\n",
       "      <th>readibility_indice</th>\n",
       "      <th>token_var_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The average american tosses 300 pounds of food...</td>\n",
       "      <td>0.61130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.182469</td>\n",
       "      <td>8.127310</td>\n",
       "      <td>83.020667</td>\n",
       "      <td>3.421333</td>\n",
       "      <td>1868.867005</td>\n",
       "      <td>8.359067</td>\n",
       "      <td>121.663282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No man is an island,Entire of itself;Every man...</td>\n",
       "      <td>0.63692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.309611</td>\n",
       "      <td>8.924134</td>\n",
       "      <td>103.139667</td>\n",
       "      <td>4.408889</td>\n",
       "      <td>731.455143</td>\n",
       "      <td>4.554444</td>\n",
       "      <td>47.248416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Things We Carry on the Sea We carry tears in o...</td>\n",
       "      <td>0.41261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.236131</td>\n",
       "      <td>10.464039</td>\n",
       "      <td>93.286130</td>\n",
       "      <td>3.751739</td>\n",
       "      <td>1585.372476</td>\n",
       "      <td>6.847989</td>\n",
       "      <td>74.398687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spanglish pues estoy creando Spanglish bi-cult...</td>\n",
       "      <td>0.34872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.406810</td>\n",
       "      <td>8.682517</td>\n",
       "      <td>5.701538</td>\n",
       "      <td>5.394872</td>\n",
       "      <td>2648.932444</td>\n",
       "      <td>18.885385</td>\n",
       "      <td>177.571748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Oh, you’ll love working here. Nobody treats y...</td>\n",
       "      <td>0.49582</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.120549</td>\n",
       "      <td>7.531277</td>\n",
       "      <td>80.394923</td>\n",
       "      <td>3.212308</td>\n",
       "      <td>5886.939046</td>\n",
       "      <td>6.865385</td>\n",
       "      <td>256.780542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  NU_PARAM_B  has_img  \\\n",
       "0  The average american tosses 300 pounds of food...     0.61130      1.0   \n",
       "1  No man is an island,Entire of itself;Every man...     0.63692      0.0   \n",
       "2  Things We Carry on the Sea We carry tears in o...     0.41261      0.0   \n",
       "3  Spanglish pues estoy creando Spanglish bi-cult...     0.34872      0.0   \n",
       "4  “Oh, you’ll love working here. Nobody treats y...     0.49582      1.0   \n",
       "\n",
       "   adapted_dalechall  brunet_indice  flesch_indice  gunning_fox_indice  \\\n",
       "0           4.182469       8.127310      83.020667            3.421333   \n",
       "1           4.309611       8.924134     103.139667            4.408889   \n",
       "2           4.236131      10.464039      93.286130            3.751739   \n",
       "3           4.406810       8.682517       5.701538            5.394872   \n",
       "4           4.120549       7.531277      80.394923            3.212308   \n",
       "\n",
       "   honore_statistics  readibility_indice  token_var_idx  \n",
       "0        1868.867005            8.359067     121.663282  \n",
       "1         731.455143            4.554444      47.248416  \n",
       "2        1585.372476            6.847989      74.398687  \n",
       "3        2648.932444           18.885385     177.571748  \n",
       "4        5886.939046            6.865385     256.780542  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2023_final.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
